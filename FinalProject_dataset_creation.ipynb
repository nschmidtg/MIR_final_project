{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject_dataset_creation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHcA-UYHmkpK"
      },
      "source": [
        "# PodcastMix subset\n",
        "\n",
        "In this dataset I compile and create all the directory structure + the metadata for the dataset. The dataset is created from a subset of the VCTK dataset (10 speakers, ~7.000 audio files) + ~1.700 most popular and featured songs from the Jamendo App.\n",
        "\n",
        "This notebook downloads the already compiled subset of the VCTK and does all the job of querying the Jamnedo API to obtain the metadata and then download all the 1.700 flac files. Because of this, the excecution of this notebook take about 4 hours, only to prepatare the dataset. You are free to run all the cells of the notebook to compile the dataset for your own and to tweak the parameters to prefer. However, if you want to save time, I have uploaded the compiled PodcastMix and in the notebook called 'FinalProject_audio_process' I do all the training and evaluation tasks, assuming that the dataset already exists.\n",
        "\n",
        "Feel free to download the compiled version directly ~XXGb or to run this notebook to compile it by yourself, and then go to the mentioned notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iipVJEvgrmb"
      },
      "source": [
        "# Connect to Drive \n",
        "the data this notebook will download its about 2Gb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZJMxmyhplP"
      },
      "source": [
        "# change the path to wherever you want to create the podcastmix in your Drive\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/MIR/FinalProject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bj7UC-JcK1-"
      },
      "source": [
        "# download the VCTK Subset (collected by myself):\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VWqB07HIE-r3xngMBgZq2E-d0upKAaVE' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1VWqB07HIE-r3xngMBgZq2E-d0upKAaVE\" -O vctk-subset.zip && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7bfSXt6q84C"
      },
      "source": [
        "# download the metadata:\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ldYpoW5HTdpFkgJmyddHHKVzkKYuqpsw' -O speaker-info-subset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw8hK8YTtv4V"
      },
      "source": [
        "# unzip the subset\n",
        "!unzip vctk-subset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGIPB8N6iCuo"
      },
      "source": [
        "# install python wget tool to download songs from the Jamendo App\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_v5PwZhJky"
      },
      "source": [
        "import json\n",
        "import os.path\n",
        "import wget\n",
        "import requests"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWv4YEjRuedd"
      },
      "source": [
        "%mkdir Jamendo"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTugcCYUugzR"
      },
      "source": [
        "%cd Jamendo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6tV0jnaz3mb"
      },
      "source": [
        "%mkdir music"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRIrbZ5GhgDB"
      },
      "source": [
        "def download_metadata_from_jamendo(n_pages = 10):\n",
        "  \"\"\"\n",
        "    Using your token from Jamendo, it queries the API to get the metadata\n",
        "    from the most popular & featured songs. The metadata.json generated file will\n",
        "    be used to download the files later. Files are downloaded in mp3, otherwise the \n",
        "    dataset get too big.\n",
        "\n",
        "    n_pages: number of pages to retrieve. Each page retrives 200 songs\n",
        "  \"\"\"\n",
        "  # your Jamendo API token here. You can get your own from https://devportal.jamendo.com/admin\n",
        "  client_id = '08bac555'\n",
        "\n",
        "  json_keys = {}\n",
        "  last_offset = 1\n",
        "  counter = 0\n",
        "  for i in range(n_pages):\n",
        "    offset = (i * 200)\n",
        "    r =requests.get('https://api.jamendo.com/v3.0/tracks/?client_id='+client_id+'&format=json&boost=popularity_total&audiodlformat=mp31&featured=true&limit=200&offset='+str(offset)+'&include=licenses')\n",
        "    songs = json.loads(r.text)['results']\n",
        "    for song in songs:\n",
        "      if(song['audiodownload_allowed']):\n",
        "        id = song['id']\n",
        "        json_keys[id] = song\n",
        "        last_offset += 1\n",
        "\n",
        "  file_name = 'metadata.json'\n",
        "  with open(file_name, 'w') as outfile:\n",
        "      json.dump(json_keys, outfile)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaWYMUgTyfQq"
      },
      "source": [
        "def download_tracks_from_jamendo():\n",
        "  with open('metadata.json') as file:\n",
        "    json_file = json.load(file)\n",
        "\n",
        "  errors = {}\n",
        "  counter = 0\n",
        "  for song_id in json_file:\n",
        "    song = json_file.get(song_id)\n",
        "    file_name = song_id + '.mp3'\n",
        "    url = song['audiodownload']\n",
        "    if not os.path.isfile('music/' + file_name):\n",
        "      try:\n",
        "        filename = wget.download(url, out='music/' + file_name)\n",
        "      except:\n",
        "        errors[song['id']]=song\n",
        "    counter +=1\n",
        "\n",
        "  with open('errors.json', 'w') as outfile:\n",
        "      json.dump(errors, outfile)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HudyOlxSvpHo"
      },
      "source": [
        "download_metadata_from_jamendo(n_pages = 10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYRn-nPoyxyR"
      },
      "source": [
        "download_tracks_from_jamendo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWpXwCLcDZ0W"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRD0ECONEfGL"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSlyA_HxIoZE"
      },
      "source": [
        "from shutil import copyfile\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "import torchaudio"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rh2T29pIvdy"
      },
      "source": [
        "# declare the path of speech and music\n",
        "# modify if necesary:\n",
        "speech_path = \"subset\"\n",
        "speech_metadata_path = \"speaker-info-subset.txt\"\n",
        "\n",
        "music_path = \"Jamendo/music\"\n",
        "music_metadata_path = \"Jamendo/metadata.json\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ7ei2lIzr5"
      },
      "source": [
        "# create files structure\n",
        "if not os.path.exists('podcastmix'):\n",
        "        os.makedirs('podcastmix')\n",
        "\n",
        "def create_folder_structure(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    if not os.path.exists(path + '/music'):\n",
        "        os.makedirs(path + '/music')\n",
        "    if not os.path.exists(path + '/speech'):\n",
        "        os.makedirs(path + '/speech')\n",
        "\n",
        "# create files structure\n",
        "train_path = \"podcastmix/train\"\n",
        "create_folder_structure(train_path)\n",
        "\n",
        "val_path = \"podcastmix/val\"\n",
        "create_folder_structure(val_path)\n",
        "\n",
        "test_path = \"podcastmix/test\"\n",
        "create_folder_structure(test_path)\n",
        "\n",
        "# create the metadata directory\n",
        "if not os.path.exists('podcastmix/metadata'):\n",
        "    os.makedirs('podcastmix/metadata')\n",
        "if not os.path.exists('podcastmix/metadata/train'):\n",
        "    os.makedirs('podcastmix/metadata/train')\n",
        "if not os.path.exists('podcastmix/metadata/val'):\n",
        "    os.makedirs('podcastmix/metadata/val')\n",
        "if not os.path.exists('podcastmix/metadata/test'):\n",
        "    os.makedirs('podcastmix/metadata/test')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AZt4KKI9hx"
      },
      "source": [
        "def create_csv_metadata(csv_path, headers):\n",
        "    with open(csv_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "# create the train csv file\n",
        "speech_headers = [\n",
        "    \"speech_ID\",\n",
        "    \"speaker_id\",\n",
        "    \"speaker_age\",\n",
        "    \"speaker_gender\",\n",
        "    \"speaker_accent\",\n",
        "    \"speech_path\",\n",
        "    \"length\"\n",
        "    ]\n",
        "music_headers = [\n",
        "    \"music_ID\",\n",
        "    \"jamendo_id\",\n",
        "    \"name\",\n",
        "    \"artist_name\",\n",
        "    \"album_name\",\n",
        "    \"license_ccurl\",\n",
        "    \"releasedate\",\n",
        "    \"music_path\",\n",
        "    \"length\"\n",
        "    ]\n",
        "\n",
        "csv_path_tr_s = 'podcastmix/metadata/train/speech.csv'\n",
        "csv_path_tr_m = 'podcastmix/metadata/train/music.csv'\n",
        "csv_path_va_s = 'podcastmix/metadata/val/speech.csv'\n",
        "csv_path_va_m = 'podcastmix/metadata/val/music.csv'\n",
        "csv_path_te_s = 'podcastmix/metadata/test/speech.csv'\n",
        "csv_path_te_m = 'podcastmix/metadata/test/music.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVZgNDmy3P7"
      },
      "source": [
        "create_csv_metadata(csv_path_tr_s, speech_headers)\n",
        "create_csv_metadata(csv_path_tr_m, music_headers)\n",
        "create_csv_metadata(csv_path_va_s, speech_headers)\n",
        "create_csv_metadata(csv_path_va_m, music_headers)\n",
        "create_csv_metadata(csv_path_te_s, speech_headers)\n",
        "create_csv_metadata(csv_path_te_m, music_headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVUs1Qn_JJIQ"
      },
      "source": [
        "# initialize the random seed\n",
        "random.seed(1)\n",
        "\n",
        "# determine the train/test partition\n",
        "train_prop = 0.8\n",
        "val_prop = 0.1\n",
        "test_prop = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3MGHVBW7tuv"
      },
      "source": [
        "# Initialize usefull arrays\n",
        "counter = 0\n",
        "speech_train_set = []\n",
        "speech_val_set = []\n",
        "speech_test_set = []\n",
        "music_train_set = []\n",
        "music_val_set = []\n",
        "music_test_set = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi0NQt6wn4PC"
      },
      "source": [
        "# process music files\n",
        "# open music metadata\n",
        "with open(music_metadata_path) as file:\n",
        "    json_file = json.load(file)\n",
        "\n",
        "# shuffle music\n",
        "keys = list(json_file.keys())\n",
        "random.shuffle(keys)\n",
        "errors = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyq2_2UuJQj7"
      },
      "source": [
        "# read the json and start copying the files to the respective directory.\n",
        "# at the same time the csv files are being filled.\n",
        "for song_id in keys:\n",
        "    song = json_file.get(song_id)\n",
        "    \n",
        "    try:\n",
        "        audio_info = torchaudio.info(music_path + '/' + song['id'] + '.mp3')\n",
        "        print(counter, '/', len(keys))\n",
        "        channels = audio_info.num_channels\n",
        "        if channels == 2:\n",
        "            if counter < int(train_prop * len(keys)):\n",
        "                # train\n",
        "                destination = train_path + '/music/' + song['id'] + '.mp3'\n",
        "                # audio = librosa.load(music_path + '/' + song['id'] + '.mp3', sr=44100, mono=False)[0]\n",
        "                # sf.write(destination, audio.T, samplerate=44100)\n",
        "                copyfile(music_path + '/' + song['id'] + '.mp3', destination)\n",
        "                song['local_path'] = destination\n",
        "                music_train_set.append(song)\n",
        "                csv_path = 'podcastmix/metadata/train/music.csv'\n",
        "            elif counter >= int(train_prop * len(keys)) and counter < int((train_prop + val_prop) * len(keys)):\n",
        "                # val\n",
        "                destination = val_path + '/music/' + song['id'] + '.mp3'\n",
        "                # audio = librosa.load(music_path + '/' + song['id'] + '.mp3', sr=44100, mono=False)[0]\n",
        "                # sf.write(destination, audio.T, samplerate=44100)\n",
        "                copyfile(music_path + '/' + song['id'] + '.mp3', destination)\n",
        "                song['local_path'] = destination\n",
        "                music_val_set.append(song)\n",
        "                csv_path = 'podcastmix/metadata/val/music.csv'\n",
        "            else:\n",
        "                # test\n",
        "                destination = test_path + '/music/' + song['id'] + '.mp3'\n",
        "                # audio = librosa.load(music_path + '/' + song['id'] + '.mp3', sr=44100, mono=False)[0]\n",
        "                # sf.write(destination, audio.T, samplerate=44100)\n",
        "                copyfile(music_path + '/' + song['id'] + '.mp3', destination)\n",
        "                song['local_path'] = destination\n",
        "                music_test_set.append(song)\n",
        "                csv_path = 'podcastmix/metadata/test/music.csv'\n",
        "            with open(csv_path, 'a', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                song_length = audio_info.num_frames\n",
        "                writer.writerow([song['id'],song['id'],song['name'].replace(',',''),song['artist_name'].replace(',',''),song['album_name'].replace(',',''),song['license_ccurl'],song['releasedate'],song['local_path'] ,song_length])\n",
        "        else:\n",
        "            errors.append(song_id)\n",
        "    except:\n",
        "        errors.append(song_id)\n",
        "    counter +=1\n",
        "print('errores',errors)\n",
        "print('keys',keys[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsrZQmCxp9oN"
      },
      "source": [
        "!pip install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvOOiF7BJaih"
      },
      "source": [
        "# process speech files\n",
        "speech_files = np.array([])\n",
        "for path, subdirs, files in os.walk(speech_path):\n",
        "   for name in files:\n",
        "       speech_files = np.append(speech_files, os.path.join(path, name))\n",
        "\n",
        "# shuffle speech\n",
        "np.random.shuffle(speech_files)\n",
        "\n",
        "# read speech metadata.txt\n",
        "\n",
        "# get the speakers metadata from the csv:\n",
        "speaker_params = {}\n",
        "s_m = open(speech_metadata_path, 'r')\n",
        "lines = s_m.readlines()\n",
        "count = 0\n",
        "for line in lines:\n",
        "   if count != 0:\n",
        "       #skip headers\n",
        "       cols = re.split('\\s+', line)\n",
        "       speaker_params[cols[0]] = {'speaker_id':cols[0],'speaker_age':cols[1],'speaker_gender':cols[2],'speaker_accent':cols[3]}\n",
        "   count += 1\n",
        "\n",
        "# iterate over the speakers downsampling and normalizing the audio.\n",
        "# The new 44.1hKz versions are then written in the respective directory\n",
        "# inside the podcastmix. the metadata csv for the podcastmix is also filled\n",
        "counter = 0\n",
        "for speech_path_dir in speech_files:\n",
        "    print(counter, '/', len(speech_files))\n",
        "    if counter < int(train_prop * len(speech_files)):\n",
        "        # train\n",
        "        destination = train_path + '/speech/' + speech_path_dir.split('/')[-1].split('.')[0] + '.mp3'\n",
        "        # resample from 48kHz -> 44.1kHz\n",
        "        audio, original_sr = torchaudio.load(speech_path_dir, normalize=True)\n",
        "        resampled_audio = torchaudio.transforms.Resample(original_sr, 44100)(audio)\n",
        "        torchaudio.save(filepath = destination, src = resampled_audio, sample_rate=44100)\n",
        "        # copyfile(speech_path_dir, destination)\n",
        "        speech_train_set.append(destination)\n",
        "        csv_path = 'podcastmix/metadata/train/speech.csv'\n",
        "    elif counter >= int(train_prop * len(speech_files)) and counter < int((train_prop + val_prop) * len(speech_files)):\n",
        "        # val\n",
        "        destination = val_path + '/speech/' + speech_path_dir.split('/')[-1].split('.')[0] + '.mp3'\n",
        "        # resample from 48kHz -> 44.1kHz\n",
        "        audio, original_sr = torchaudio.load(speech_path_dir, normalize=True)\n",
        "        resampled_audio = torchaudio.transforms.Resample(original_sr, 44100)(audio)\n",
        "        torchaudio.save(filepath = destination, src = resampled_audio, sample_rate=44100)\n",
        "        # copyfile(speech_path_dir, destination)\n",
        "        speech_val_set.append(destination)\n",
        "        csv_path = 'podcastmix/metadata/val/speech.csv'\n",
        "    else:\n",
        "        # test\n",
        "        destination = test_path + '/speech/' + speech_path_dir.split('/')[-1].split('.')[0] + '.mp3'\n",
        "        # resample from 48kHz -> 44.1kHz\n",
        "        audio, original_sr = torchaudio.load(speech_path_dir, normalize=True)\n",
        "        resampled_audio = torchaudio.transforms.Resample(original_sr, 44100)(audio)\n",
        "        torchaudio.save(filepath = destination, src = resampled_audio, sample_rate=44100)\n",
        "        # copyfile(speech_path_dir, destination)\n",
        "        speech_test_set.append(destination)\n",
        "        csv_path = 'podcastmix/metadata/test/speech.csv'\n",
        "    with open(csv_path, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        audio = torchaudio.info(destination)\n",
        "        element_length = audio.num_frames\n",
        "        speech_cmp = destination.split('/')[-1].split('_')\n",
        "        params = speaker_params[speech_cmp[0]]\n",
        "        writer.writerow([speech_cmp[1]+'_'+speech_cmp[2].split('.')[0], speech_cmp[0], params['speaker_age'], params['speaker_gender'], params['speaker_accent'].replace(',',''), destination, element_length])\n",
        "    counter += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP16ySVSwdgx"
      },
      "source": [
        "# Check dataset consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65RIqfrwf5H"
      },
      "source": [
        "import os\n",
        "from csv import reader\n",
        "\n",
        "def check_files_against_csv(csv_path, files_path, index_of_path_in_csv = 7):\n",
        "    not_missing = []\n",
        "\n",
        "    with open(csv_path, 'r') as read_obj:\n",
        "        csv_reader = reader(read_obj)\n",
        "        header = next(csv_reader)\n",
        "        # Check file as empty\n",
        "        if header != None:\n",
        "            # Iterate over each row after the header in the csv\n",
        "            for row in csv_reader:\n",
        "                # row variable is a list that represents a row in csv\n",
        "                path = row[index_of_path_in_csv]\n",
        "                if os.path.isfile(path):\n",
        "                    not_missing.append(path.split('/')[3])\n",
        "                else:\n",
        "                    print(\"im not file\", path)\n",
        "\n",
        "    onlyfiles = [f for f in listdir(files_path) if isfile(join(files_path, f))]\n",
        "    print('Check diff between:', files_path, csv_path)\n",
        "    diff = list(set(onlyfiles) - set(not_missing))\n",
        "    print('List of files minus list in csv:', diff)\n",
        "    diff2 = list(set(not_missing) - set(onlyfiles))\n",
        "    print('List in csv minus list of files:', diff2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0k-y363wuIT"
      },
      "source": [
        "# check consistency of the dataset:\n",
        "files_path = 'podcastmix/test/music'\n",
        "csv_path = 'podcastmix/metadata/test/music.csv'\n",
        "check_files_against_csv(csv_path, files_path, 7)\n",
        "\n",
        "files_path = 'podcastmix/val/music'\n",
        "csv_path = 'podcastmix/metadata/val/music.csv'\n",
        "check_files_against_csv(csv_path, files_path, 7)\n",
        "\n",
        "files_path = 'podcastmix/train/music'\n",
        "csv_path = 'podcastmix/metadata/train/music.csv'\n",
        "check_files_against_csv(csv_path, files_path, 7)\n",
        "\n",
        "files_path = 'podcastmix/train/speech'\n",
        "csv_path = 'podcastmix/metadata/train/speech.csv'\n",
        "check_files_against_csv(csv_path, files_path, 5)\n",
        "\n",
        "files_path = 'podcastmix/val/speech'\n",
        "csv_path = 'podcastmix/metadata/val/speech.csv'\n",
        "check_files_against_csv(csv_path, files_path, 5)\n",
        "\n",
        "files_path = 'podcastmix/test/speech'\n",
        "csv_path = 'podcastmix/metadata/test/speech.csv'\n",
        "check_files_against_csv(csv_path, files_path, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJuVBmqmSlkG"
      },
      "source": [
        "#Dataset ready! \n",
        "Now go to the FinalProject_audio_processing notebook to train and evaluate the models over this dataset.\n"
      ]
    }
  ]
}